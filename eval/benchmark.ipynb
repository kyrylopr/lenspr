{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LensPR Benchmark\n\nMeasures Claude's performance with and without LensPR tools on code analysis tasks.\n\n**Metrics:** Token usage, iterations, completion rate\n\nRun: `make benchmark`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '..')\n\nimport json\nimport os\nfrom pathlib import Path\nfrom dataclasses import dataclass, field, asdict\nfrom datetime import datetime\nfrom typing import Any\n\n# Load API key from .env\nfrom dotenv import load_dotenv\nload_dotenv()\n\nimport anthropic\nimport lenspr\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Initialize\nPROJECT_ROOT = Path('..').resolve()\nRESULTS_DIR = Path('results')\nRESULTS_DIR.mkdir(exist_ok=True)\n\n# Config\nMAX_ITERATIONS = 50\n\nclient = anthropic.Anthropic()\nctx = lenspr.init(str(PROJECT_ROOT))\n\nprint(f\"Project: {PROJECT_ROOT}\")\nprint(f\"Max iterations: {MAX_ITERATIONS}\")\nprint(f\"Results dir: {RESULTS_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics & Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Stores results from a single benchmark run.\"\"\"\n",
    "    task_id: str\n",
    "    mode: str\n",
    "    total_input_tokens: int = 0\n",
    "    total_output_tokens: int = 0\n",
    "    iterations: int = 0\n",
    "    tool_calls: list = field(default_factory=list)\n",
    "    tool_call_count: int = 0\n",
    "    completed: bool = False\n",
    "    error: str | None = None\n",
    "    started_at: str = \"\"\n",
    "    finished_at: str = \"\"\n",
    "    duration_seconds: float = 0.0\n",
    "    messages: list = field(default_factory=list)\n",
    "    \n",
    "    # Track tokens per iteration for visualization\n",
    "    tokens_per_iteration: list = field(default_factory=list)\n",
    "    \n",
    "    def save(self, path: Path):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(asdict(self), f, indent=2, default=str)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: Path) -> 'BenchmarkResult':\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        return cls(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard tools (without LensPR)\n",
    "STANDARD_TOOLS = [\n",
    "    {\n",
    "        \"name\": \"read_file\",\n",
    "        \"description\": \"Read a file from the filesystem. Returns the file content.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"path\": {\"type\": \"string\", \"description\": \"Path to the file to read\"}\n",
    "            },\n",
    "            \"required\": [\"path\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"write_file\",\n",
    "        \"description\": \"Write content to a file. Creates or overwrites the file.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"path\": {\"type\": \"string\", \"description\": \"Path to the file\"},\n",
    "                \"content\": {\"type\": \"string\", \"description\": \"Content to write\"}\n",
    "            },\n",
    "            \"required\": [\"path\", \"content\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_files\",\n",
    "        \"description\": \"Search for a pattern in files using grep. Returns matching lines with file paths.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"pattern\": {\"type\": \"string\", \"description\": \"Pattern to search for\"},\n",
    "                \"path\": {\"type\": \"string\", \"description\": \"Directory to search in\", \"default\": \".\"}\n",
    "            },\n",
    "            \"required\": [\"pattern\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"list_files\",\n",
    "        \"description\": \"List files in a directory.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"path\": {\"type\": \"string\", \"description\": \"Directory path\", \"default\": \".\"}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"run_tests\",\n",
    "        \"description\": \"Run pytest on the project. Returns test output.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"path\": {\"type\": \"string\", \"description\": \"Test path\", \"default\": \"tests/\"}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"task_complete\",\n",
    "        \"description\": \"Call this when the task is complete. Provide a summary of what was done.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"summary\": {\"type\": \"string\", \"description\": \"Summary of completed work\"}\n",
    "            },\n",
    "            \"required\": [\"summary\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "LENSPR_TOOLS = lenspr.get_claude_tools() + STANDARD_TOOLS\n",
    "print(f\"Standard tools: {len(STANDARD_TOOLS)}, LensPR tools: {len(LENSPR_TOOLS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def handle_standard_tool(name: str, inputs: dict) -> str:\n",
    "    try:\n",
    "        if name == \"read_file\":\n",
    "            path = PROJECT_ROOT / inputs[\"path\"]\n",
    "            if not path.exists():\n",
    "                return f\"Error: File not found: {inputs['path']}\"\n",
    "            return path.read_text()\n",
    "        elif name == \"write_file\":\n",
    "            path = PROJECT_ROOT / inputs[\"path\"]\n",
    "            path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            path.write_text(inputs[\"content\"])\n",
    "            return f\"Successfully wrote {len(inputs['content'])} characters to {inputs['path']}\"\n",
    "        elif name == \"search_files\":\n",
    "            search_path = PROJECT_ROOT / inputs.get(\"path\", \".\")\n",
    "            result = subprocess.run(\n",
    "                [\"grep\", \"-rn\", inputs[\"pattern\"], str(search_path)],\n",
    "                capture_output=True, text=True, timeout=30\n",
    "            )\n",
    "            output = result.stdout or \"No matches found\"\n",
    "            if len(output) > 10000:\n",
    "                output = output[:10000] + f\"\\n... (truncated)\"\n",
    "            return output\n",
    "        elif name == \"list_files\":\n",
    "            path = PROJECT_ROOT / inputs.get(\"path\", \".\")\n",
    "            if not path.exists():\n",
    "                return f\"Error: Directory not found\"\n",
    "            files = [str(p.relative_to(PROJECT_ROOT)) for p in sorted(path.rglob(\"*.py\"))[:100]]\n",
    "            return \"\\n\".join(files) if files else \"No Python files found\"\n",
    "        elif name == \"run_tests\":\n",
    "            result = subprocess.run(\n",
    "                [\"python\", \"-m\", \"pytest\", inputs.get(\"path\", \"tests/\"), \"-v\", \"--tb=short\"],\n",
    "                capture_output=True, text=True, timeout=120, cwd=str(PROJECT_ROOT)\n",
    "            )\n",
    "            output = result.stdout + result.stderr\n",
    "            return output[:15000] if len(output) > 15000 else output\n",
    "        elif name == \"task_complete\":\n",
    "            return f\"TASK_COMPLETE: {inputs.get('summary', 'No summary')}\"\n",
    "        else:\n",
    "            return f\"Unknown tool: {name}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error executing {name}: {str(e)}\"\n",
    "\n",
    "def handle_tool_call(name: str, inputs: dict) -> str:\n",
    "    if name.startswith(\"lens_\"):\n",
    "        result = lenspr.handle_tool(name, inputs)\n",
    "        return json.dumps(result, indent=2, default=str)\n",
    "    else:\n",
    "        return handle_standard_tool(name, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic Loop (with token tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(\n",
    "    task: str,\n",
    "    tools: list,\n",
    "    system_prompt: str,\n",
    "    max_iterations: int = MAX_ITERATIONS,\n",
    "    model: str = \"claude-sonnet-4-20250514\"\n",
    ") -> BenchmarkResult:\n",
    "    \n",
    "    result = BenchmarkResult(\n",
    "        task_id=\"\",\n",
    "        mode=\"with_lenspr\" if any(t[\"name\"].startswith(\"lens_\") for t in tools) else \"without_lenspr\",\n",
    "        started_at=datetime.now().isoformat()\n",
    "    )\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": task}]\n",
    "    cumulative_input = 0\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        result.iterations += 1\n",
    "        print(f\"\\n--- Iteration {result.iterations} ---\")\n",
    "        \n",
    "        response = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=4096,\n",
    "            system=system_prompt,\n",
    "            tools=tools,\n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        # Track tokens\n",
    "        result.total_input_tokens += response.usage.input_tokens\n",
    "        result.total_output_tokens += response.usage.output_tokens\n",
    "        cumulative_input += response.usage.input_tokens\n",
    "        \n",
    "        # Track per-iteration for visualization\n",
    "        result.tokens_per_iteration.append({\n",
    "            \"iteration\": result.iterations,\n",
    "            \"input\": response.usage.input_tokens,\n",
    "            \"output\": response.usage.output_tokens,\n",
    "            \"cumulative_input\": cumulative_input\n",
    "        })\n",
    "        \n",
    "        print(f\"Tokens: +{response.usage.input_tokens} in, +{response.usage.output_tokens} out (cumulative: {cumulative_input:,})\")\n",
    "        \n",
    "        assistant_content = response.content\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assistant_content})\n",
    "        \n",
    "        for block in assistant_content:\n",
    "            if hasattr(block, 'text'):\n",
    "                text = block.text[:150] + '...' if len(block.text) > 150 else block.text\n",
    "                print(f\"Claude: {text}\")\n",
    "        \n",
    "        if response.stop_reason == \"tool_use\":\n",
    "            tool_results = []\n",
    "            \n",
    "            for block in assistant_content:\n",
    "                if block.type == \"tool_use\":\n",
    "                    tool_name = block.name\n",
    "                    tool_input = block.input\n",
    "                    \n",
    "                    result.tool_calls.append({\"name\": tool_name, \"input\": tool_input})\n",
    "                    result.tool_call_count += 1\n",
    "                    \n",
    "                    print(f\"Tool: {tool_name}\")\n",
    "                    \n",
    "                    if tool_name == \"task_complete\":\n",
    "                        result.completed = True\n",
    "                        result.finished_at = datetime.now().isoformat()\n",
    "                        result.messages = [{\"role\": m[\"role\"], \"content\": str(m[\"content\"])[:500]} for m in messages]\n",
    "                        print(f\"\\nâœ… Task completed in {result.iterations} iterations!\")\n",
    "                        return result\n",
    "                    \n",
    "                    tool_result = handle_tool_call(tool_name, tool_input)\n",
    "                    tool_results.append({\n",
    "                        \"type\": \"tool_result\",\n",
    "                        \"tool_use_id\": block.id,\n",
    "                        \"content\": tool_result[:5000]\n",
    "                    })\n",
    "            \n",
    "            messages.append({\"role\": \"user\", \"content\": tool_results})\n",
    "        \n",
    "        elif response.stop_reason == \"end_turn\":\n",
    "            print(\"Claude stopped. Prompting to continue...\")\n",
    "            messages.append({\"role\": \"user\", \"content\": \"Please continue. Use task_complete when done.\"})\n",
    "    \n",
    "    result.error = f\"Max iterations ({max_iterations}) reached\"\n",
    "    result.finished_at = datetime.now().isoformat()\n",
    "    result.messages = [{\"role\": m[\"role\"], \"content\": str(m[\"content\"])[:500]} for m in messages]\n",
    "    print(f\"\\nâŒ Max iterations reached\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompts & Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_WITHOUT_LENSPR = \"\"\"\n",
    "You are a code assistant working on a Python project.\n",
    "\n",
    "Available tools:\n",
    "- read_file: Read file contents\n",
    "- write_file: Write to files\n",
    "- search_files: Search with grep\n",
    "- list_files: List Python files\n",
    "- run_tests: Run pytest\n",
    "- task_complete: Call when done\n",
    "\n",
    "Project root: lenspr/ (a Python code analysis library)\n",
    "\n",
    "Rules:\n",
    "1. Complete the task fully\n",
    "2. Update ALL affected files\n",
    "3. Call task_complete when done\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_WITH_LENSPR = \"\"\"\n",
    "You are a code assistant working on a Python project with LensPR code graph tools.\n",
    "\n",
    "IMPORTANT: Use LensPR tools for Python code:\n",
    "- lens_context: Get function + callers + callees + tests (use this first!)\n",
    "- lens_check_impact: ALWAYS check before modifying code\n",
    "- lens_find_usages: Find all references to a function\n",
    "- lens_search: Search by name\n",
    "\n",
    "Standard tools: read_file, write_file, search_files, list_files, run_tests, task_complete\n",
    "\n",
    "Rules:\n",
    "1. Use lens_context to understand functions\n",
    "2. Complete the task fully\n",
    "3. Call task_complete when done\n",
    "\"\"\"\n",
    "\n",
    "TASKS = {\n",
    "    \"task1_understand\": {\n",
    "        \"name\": \"Understand a function\",\n",
    "        \"prompt\": \"\"\"Analyze the function `check_impact` in lenspr/graph.py.\n",
    "\n",
    "Tell me:\n",
    "1. What does it do?\n",
    "2. What functions call it?\n",
    "3. What functions does it call?\n",
    "4. What tests cover it?\n",
    "\n",
    "Call task_complete with your analysis.\"\"\"\n",
    "    },\n",
    "    \"task2_find_usages\": {\n",
    "        \"name\": \"Find all usages\",\n",
    "        \"prompt\": \"\"\"Find ALL places where the function `get_node` from lenspr/database.py is used.\n",
    "\n",
    "List:\n",
    "1. All files that import it\n",
    "2. All functions that call it\n",
    "3. All tests that use it\n",
    "\n",
    "Call task_complete with the complete list.\"\"\"\n",
    "    },\n",
    "    \"task3_safe_change\": {\n",
    "        \"name\": \"Safe code change\",\n",
    "        \"prompt\": \"\"\"I want to add a new parameter `include_external: bool = False` to the function \n",
    "`get_connections` in lenspr/database.py.\n",
    "\n",
    "Before making ANY changes:\n",
    "1. Analyze what will be affected by this change\n",
    "2. List all callers that might need updates\n",
    "3. Tell me the severity/risk of this change\n",
    "\n",
    "DO NOT actually make the change - just analyze the impact.\n",
    "Call task_complete with your analysis.\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Tasks: {list(TASKS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(task_id: str, with_lenspr: bool) -> BenchmarkResult:\n",
    "    task = TASKS[task_id]\n",
    "    mode = \"with_lenspr\" if with_lenspr else \"without_lenspr\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running: {task['name']} ({mode})\")\n",
    "    print(f\"Max iterations: {MAX_ITERATIONS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    tools = LENSPR_TOOLS if with_lenspr else STANDARD_TOOLS\n",
    "    system = SYSTEM_PROMPT_WITH_LENSPR if with_lenspr else SYSTEM_PROMPT_WITHOUT_LENSPR\n",
    "    \n",
    "    result = run_agent(\n",
    "        task=task[\"prompt\"],\n",
    "        tools=tools,\n",
    "        system_prompt=system,\n",
    "        max_iterations=MAX_ITERATIONS\n",
    "    )\n",
    "    \n",
    "    result.task_id = task_id\n",
    "    result.mode = mode\n",
    "    \n",
    "    result_path = RESULTS_DIR / f\"{task_id}_{mode}.json\"\n",
    "    result.save(result_path)\n",
    "    print(f\"Saved: {result_path}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Understand a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_t1_without = run_benchmark(\"task1_understand\", with_lenspr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_t1_with = run_benchmark(\"task1_understand\", with_lenspr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Find All Usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_t2_without = run_benchmark(\"task2_find_usages\", with_lenspr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_t2_with = run_benchmark(\"task2_find_usages\", with_lenspr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Safe Code Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_t3_without = run_benchmark(\"task3_safe_change\", with_lenspr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_t3_with = run_benchmark(\"task3_safe_change\", with_lenspr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Results & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_results() -> dict:\n",
    "    results = {}\n",
    "    for path in RESULTS_DIR.glob(\"*.json\"):\n",
    "        result = BenchmarkResult.load(path)\n",
    "        key = f\"{result.task_id}_{result.mode}\"\n",
    "        results[key] = result\n",
    "    return results\n",
    "\n",
    "results = load_all_results()\n",
    "print(f\"Loaded {len(results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart 1: Iterations to Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "tasks = list(TASKS.keys())\n",
    "task_names = [TASKS[t]['name'] for t in tasks]\n",
    "x = np.arange(len(tasks))\n",
    "width = 0.35\n",
    "\n",
    "without_iters = []\n",
    "with_iters = []\n",
    "without_completed = []\n",
    "with_completed = []\n",
    "\n",
    "for task_id in tasks:\n",
    "    wo = results.get(f\"{task_id}_without_lenspr\")\n",
    "    w = results.get(f\"{task_id}_with_lenspr\")\n",
    "    \n",
    "    without_iters.append(wo.iterations if wo else 0)\n",
    "    with_iters.append(w.iterations if w else 0)\n",
    "    without_completed.append(wo.completed if wo else False)\n",
    "    with_completed.append(w.completed if w else False)\n",
    "\n",
    "bars1 = ax.bar(x - width/2, without_iters, width, label='Without LensPR', color='#ff6b6b')\n",
    "bars2 = ax.bar(x + width/2, with_iters, width, label='With LensPR', color='#4ecdc4')\n",
    "\n",
    "# Add completion markers\n",
    "for i, (completed, bar) in enumerate(zip(without_completed, bars1)):\n",
    "    marker = 'âœ“' if completed else 'âœ—'\n",
    "    color = 'green' if completed else 'red'\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, marker, \n",
    "            ha='center', va='bottom', fontsize=14, color=color)\n",
    "\n",
    "for i, (completed, bar) in enumerate(zip(with_completed, bars2)):\n",
    "    marker = 'âœ“' if completed else 'âœ—'\n",
    "    color = 'green' if completed else 'red'\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, marker, \n",
    "            ha='center', va='bottom', fontsize=14, color=color)\n",
    "\n",
    "ax.set_ylabel('Iterations')\n",
    "ax.set_title('Iterations to Complete Task (âœ“=completed, âœ—=hit limit)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(task_names, rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.axhline(y=MAX_ITERATIONS, color='gray', linestyle='--', alpha=0.5, label=f'Max ({MAX_ITERATIONS})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'chart_iterations.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart 2: Total Input Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "without_tokens = [results.get(f\"{t}_without_lenspr\").total_input_tokens if results.get(f\"{t}_without_lenspr\") else 0 for t in tasks]\n",
    "with_tokens = [results.get(f\"{t}_with_lenspr\").total_input_tokens if results.get(f\"{t}_with_lenspr\") else 0 for t in tasks]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, [t/1000 for t in without_tokens], width, label='Without LensPR', color='#ff6b6b')\n",
    "bars2 = ax.bar(x + width/2, [t/1000 for t in with_tokens], width, label='With LensPR', color='#4ecdc4')\n",
    "\n",
    "ax.set_ylabel('Input Tokens (thousands)')\n",
    "ax.set_title('Total Context Tokens Consumed')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(task_names, rotation=15, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars1, without_tokens):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, f'{val/1000:.0f}k', \n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "for bar, val in zip(bars2, with_tokens):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, f'{val/1000:.0f}k', \n",
    "            ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'chart_tokens.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart 3: Cumulative Tokens Over Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, task_id in enumerate(tasks):\n",
    "    ax = axes[idx]\n",
    "    task_name = TASKS[task_id]['name']\n",
    "    \n",
    "    wo = results.get(f\"{task_id}_without_lenspr\")\n",
    "    w = results.get(f\"{task_id}_with_lenspr\")\n",
    "    \n",
    "    if wo and wo.tokens_per_iteration:\n",
    "        iters = [t['iteration'] for t in wo.tokens_per_iteration]\n",
    "        cumulative = [t['cumulative_input']/1000 for t in wo.tokens_per_iteration]\n",
    "        ax.plot(iters, cumulative, 'o-', color='#ff6b6b', label='Without LensPR', linewidth=2)\n",
    "        if not wo.completed:\n",
    "            ax.axvline(x=wo.iterations, color='#ff6b6b', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    if w and w.tokens_per_iteration:\n",
    "        iters = [t['iteration'] for t in w.tokens_per_iteration]\n",
    "        cumulative = [t['cumulative_input']/1000 for t in w.tokens_per_iteration]\n",
    "        ax.plot(iters, cumulative, 's-', color='#4ecdc4', label='With LensPR', linewidth=2)\n",
    "        if w.completed:\n",
    "            ax.axvline(x=w.iterations, color='#4ecdc4', linestyle=':', alpha=0.7)\n",
    "            ax.text(w.iterations, ax.get_ylim()[1]*0.9, 'âœ“', color='green', fontsize=14, ha='center')\n",
    "    \n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Cumulative Input Tokens (k)')\n",
    "    ax.set_title(task_name)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'chart_cumulative.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart 4: Tool Usage Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count tool usage\n",
    "def count_tools(results_list):\n",
    "    counts = {}\n",
    "    for r in results_list:\n",
    "        if r:\n",
    "            for tc in r.tool_calls:\n",
    "                name = tc['name']\n",
    "                counts[name] = counts.get(name, 0) + 1\n",
    "    return counts\n",
    "\n",
    "without_results = [results.get(f\"{t}_without_lenspr\") for t in tasks]\n",
    "with_results = [results.get(f\"{t}_with_lenspr\") for t in tasks]\n",
    "\n",
    "without_counts = count_tools(without_results)\n",
    "with_counts = count_tools(with_results)\n",
    "\n",
    "# Without LensPR\n",
    "ax = axes[0]\n",
    "if without_counts:\n",
    "    tools_sorted = sorted(without_counts.items(), key=lambda x: -x[1])[:10]\n",
    "    names, counts = zip(*tools_sorted)\n",
    "    colors = ['#ff6b6b' if not n.startswith('lens_') else '#4ecdc4' for n in names]\n",
    "    ax.barh(names, counts, color=colors)\n",
    "    ax.set_xlabel('Call Count')\n",
    "    ax.set_title('Without LensPR - Tool Usage')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "# With LensPR\n",
    "ax = axes[1]\n",
    "if with_counts:\n",
    "    tools_sorted = sorted(with_counts.items(), key=lambda x: -x[1])[:10]\n",
    "    names, counts = zip(*tools_sorted)\n",
    "    colors = ['#4ecdc4' if n.startswith('lens_') else '#ff6b6b' for n in names]\n",
    "    ax.barh(names, counts, color=colors)\n",
    "    ax.set_xlabel('Call Count')\n",
    "    ax.set_title('With LensPR - Tool Usage')\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'chart_tools.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_table(results: dict):\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"SUMMARY: LensPR Impact on Claude Code Performance (max_iterations=50)\")\n",
    "    print(\"=\"*90)\n",
    "    print()\n",
    "    \n",
    "    totals = {\"with\": {\"in\": 0, \"out\": 0, \"iter\": 0, \"tools\": 0, \"completed\": 0},\n",
    "              \"without\": {\"in\": 0, \"out\": 0, \"iter\": 0, \"tools\": 0, \"completed\": 0}}\n",
    "    \n",
    "    print(f\"{'Task':<25} {'Mode':<10} {'Input':>10} {'Output':>10} {'Iter':>6} {'Tools':>6} {'Done':>6}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for task_id in TASKS.keys():\n",
    "        for mode in [\"without_lenspr\", \"with_lenspr\"]:\n",
    "            key = f\"{task_id}_{mode}\"\n",
    "            if key in results:\n",
    "                r = results[key]\n",
    "                mode_short = \"WITHOUT\" if \"without\" in mode else \"WITH\"\n",
    "                done = \"âœ“\" if r.completed else \"âœ—\"\n",
    "                print(f\"{TASKS[task_id]['name']:<25} {mode_short:<10} {r.total_input_tokens:>10,} {r.total_output_tokens:>10,} {r.iterations:>6} {r.tool_call_count:>6} {done:>6}\")\n",
    "                \n",
    "                m = \"without\" if \"without\" in mode else \"with\"\n",
    "                totals[m][\"in\"] += r.total_input_tokens\n",
    "                totals[m][\"out\"] += r.total_output_tokens\n",
    "                totals[m][\"iter\"] += r.iterations\n",
    "                totals[m][\"tools\"] += r.tool_call_count\n",
    "                totals[m][\"completed\"] += 1 if r.completed else 0\n",
    "        print()\n",
    "    \n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{'TOTAL':<25} {'WITHOUT':<10} {totals['without']['in']:>10,} {totals['without']['out']:>10,} {totals['without']['iter']:>6} {totals['without']['tools']:>6} {totals['without']['completed']}/3\")\n",
    "    print(f\"{'TOTAL':<25} {'WITH':<10} {totals['with']['in']:>10,} {totals['with']['out']:>10,} {totals['with']['iter']:>6} {totals['with']['tools']:>6} {totals['with']['completed']}/3\")\n",
    "    \n",
    "    # Calculate improvements\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"IMPROVEMENTS WITH LENSPR\")\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    if totals['without']['iter'] > 0:\n",
    "        iter_savings = (totals['without']['iter'] - totals['with']['iter']) / totals['without']['iter'] * 100\n",
    "        print(f\"ðŸ“‰ Iterations: {totals['without']['iter']} â†’ {totals['with']['iter']} ({iter_savings:+.1f}%)\")\n",
    "    \n",
    "    if totals['without']['in'] > 0:\n",
    "        token_diff = (totals['with']['in'] - totals['without']['in']) / totals['without']['in'] * 100\n",
    "        print(f\"ðŸ“Š Input tokens: {totals['without']['in']:,} â†’ {totals['with']['in']:,} ({token_diff:+.1f}%)\")\n",
    "    \n",
    "    print(f\"âœ… Completion: {totals['without']['completed']}/3 â†’ {totals['with']['completed']}/3\")\n",
    "\n",
    "summary_table(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}